# ğŸ§  Neuromancer - AI Chat with Memory

A cross-platform Python application that provides an LLM interface with persistent memory capabilities and Model Context Protocol (MCP) support.

## âœ¨ Features

ğŸ¯ **Persistent Memory System**: ChromaDB-powered vector storage with four memory types (Short-term, Long-term, Episodic, Semantic) that remembers conversations and user information across sessions.

ğŸ”„ **Multiple LLM Providers**: Switch between Ollama, OpenAI, and LM Studio without losing conversation context.

ğŸ§  **Retrieval-Augmented Generation (RAG)**: Enhanced responses using relevant memories with smart prioritization for personal information.

ğŸ”Œ **MCP Integration**: WebSocket-based client for connecting to Model Context Protocol servers.

ğŸ¨ **Cross-Platform GUI**: Kivy-based interface with Material Design, supporting dark/light themes.

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Ollama installed (for local models)

### Setup
```bash
git clone <repo-url>
cd fantastic-funicular
./scripts/setup.sh
```

### Configuration
Edit `.env` for your API keys:
```bash
OPENAI_API_KEY=your_key_here  # Optional
LMSTUDIO_HOST=http://localhost:1234  # Optional
```

### Run
```bash
./scripts/run.sh
```

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ core/                    # Core functionality
â”‚   â”œâ”€â”€ chat_manager.py      # Chat orchestration and RAG integration
â”‚   â”œâ”€â”€ model_manager.py     # Provider switching and model management
â”‚   â”œâ”€â”€ rag_system.py        # Retrieval-augmented generation
â”‚   â””â”€â”€ config.py           # Configuration management
â”œâ”€â”€ providers/               # LLM provider implementations
â”‚   â”œâ”€â”€ ollama_provider.py   # Ollama local models
â”‚   â”œâ”€â”€ openai_provider.py   # OpenAI API integration
â”‚   â””â”€â”€ lmstudio_provider.py # LM Studio support
â”œâ”€â”€ memory/                  # Memory system
â”‚   â”œâ”€â”€ manager.py          # Memory operations and consolidation
â”‚   â”œâ”€â”€ vector_store.py     # ChromaDB vector storage
â”‚   â””â”€â”€ intelligent_analyzer.py # Content analysis for memory formation
â”œâ”€â”€ mcp/                     # Model Context Protocol
â”‚   â”œâ”€â”€ client.py           # WebSocket MCP client
â”‚   â””â”€â”€ manager.py          # MCP server management
â”œâ”€â”€ gui/                     # Kivy user interface
â”‚   â”œâ”€â”€ app.py              # Main application
â”‚   â””â”€â”€ screens/            # Chat, memory, and settings screens
â””â”€â”€ utils/                   # Utilities
    â”œâ”€â”€ embeddings.py       # Text embedding generation
    â””â”€â”€ logger.py           # Logging configuration
```

## ğŸ’¾ Memory System Details

The memory system uses ChromaDB for vector storage with:

- **Short-term**: Recent conversation context (24-hour default retention)
- **Long-term**: Important facts and user preferences
- **Episodic**: Specific events and experiences with timestamps
- **Semantic**: General knowledge and concepts

Features include:
- Automatic memory consolidation from short to long-term based on importance scores
- Vector similarity search with configurable thresholds
- Personal information prioritization in RAG retrieval
- Memory deduplication and contradiction filtering

## ğŸ”§ Development

### Install dev dependencies
```bash
pip install -e ".[dev]"
```

### Run tests
```bash
pytest --cov=src --cov-report=html
```

### Code formatting
```bash
black src tests
ruff src tests
mypy src
```

## ğŸ“‹ Requirements

- Python 3.10+
- ChromaDB for vector storage
- Kivy for GUI
- sentence-transformers for embeddings
- Optional: Ollama, OpenAI API key, LM Studio

## ğŸ”Œ MCP Integration

Supports WebSocket connections to MCP servers for extended functionality. Configure servers in the settings screen or via configuration files.

## âš™ï¸ Configuration

The application uses Pydantic for configuration management with support for:
- Environment variables
- JSON configuration files
- Runtime configuration updates

## ğŸ“œ License

MIT License