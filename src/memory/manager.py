"""Memory management system with intelligent retrieval and storage."""

from datetime import datetime, timedelta
from typing import Any

from src.memory import Memory, MemoryStore, MemoryType
from src.memory.intelligent_analyzer import IntelligentMemoryAnalyzer
from src.memory.vector_store import VectorMemoryStore
from src.utils.embeddings import EmbeddingGenerator
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class MemoryManager:
    """Manages memory storage, retrieval, and optimization."""

    def __init__(
        self,
        store: MemoryStore | None = None,
        embedding_generator: EmbeddingGenerator | None = None,
    ):
        """Initialize memory manager.

        Args:
            store: Memory storage backend (defaults to VectorMemoryStore)
            embedding_generator: Generator for text embeddings
        """
        self.store = store or VectorMemoryStore()
        self.embedding_generator = embedding_generator or EmbeddingGenerator()
        self.intelligent_analyzer = IntelligentMemoryAnalyzer()

        # Memory configuration
        self.short_term_duration = timedelta(hours=24)
        self.importance_threshold = 0.3
        self.consolidation_interval = timedelta(hours=6)

        # Conversation context for intelligent analysis
        self._conversation_context = []
        self._max_context_length = 10

        # Start background tasks
        self._start_background_tasks()

    def _start_background_tasks(self):
        """Start background memory management tasks."""
        # TODO: Implement memory consolidation and cleanup
        pass

    async def remember(
        self,
        content: str,
        memory_type: MemoryType = MemoryType.SHORT_TERM,
        importance: float = 0.5,
        metadata: dict[str, Any] | None = None,
        auto_classify: bool = False,
    ) -> str:
        """Store a new memory with enhanced formation logic.

        Args:
            content: The content to remember
            memory_type: Type of memory (can be auto-classified)
            importance: Importance score (0-1, can be auto-calculated)
            metadata: Additional metadata
            auto_classify: Whether to automatically classify memory type and importance

        Returns:
            Memory ID
        """
        try:
            # Generate embedding
            embedding = await self.embedding_generator.generate(content)

            # Enhanced metadata with formation context (minimal for speed)
            enhanced_metadata = metadata or {}
            if auto_classify:
                enhanced_metadata.update(
                    {
                        "formation_timestamp": datetime.now().isoformat(),
                        "content_length": len(content),
                        "formation_method": "enhanced_auto",
                    }
                )

                # Auto-classify memory type if requested
                memory_type = self._classify_memory_type(content, memory_type)
                enhanced_metadata["auto_classified_type"] = memory_type.value

                # Auto-calculate importance if requested
                importance = self._calculate_importance(content, importance)
                enhanced_metadata["auto_calculated_importance"] = importance

                # Add minimal content analysis metadata
                enhanced_metadata.update(self._analyze_content_minimal(content))
            else:
                # Minimal metadata for speed
                enhanced_metadata.update(
                    {
                        "formation_timestamp": datetime.now().isoformat(),
                        "formation_method": "manual",
                    }
                )

            # Create memory object
            memory = Memory(
                id="",  # Will be generated by store
                content=content,
                embedding=embedding,
                memory_type=memory_type,
                importance=importance,
                metadata=enhanced_metadata,
            )

            # Store memory
            memory_id = await self.store.store(memory)

            logger.info(
                f"Stored {memory_type.value} memory: {memory_id[:8]}... (importance: {importance:.2f})"
            )
            return memory_id

        except Exception as e:
            logger.error(f"Failed to remember content: {e}")
            # Don't raise - return empty string to indicate failure
            return ""

    async def intelligent_remember(
        self, content: str, conversation_context: list[str] = None
    ) -> list[str]:
        """
        Intelligently analyze content and automatically store significant memories.

        Args:
            content: The content to analyze
            conversation_context: Recent conversation messages for context

        Returns:
            List of memory IDs that were created
        """
        try:
            # Update conversation context
            if conversation_context:
                self._conversation_context = conversation_context[-self._max_context_length :]
            else:
                # Add current content to context
                self._conversation_context.append(content)
                if len(self._conversation_context) > self._max_context_length:
                    self._conversation_context = self._conversation_context[
                        -self._max_context_length :
                    ]

            # Analyze content for memory signals
            memory_signals = self.intelligent_analyzer.analyze_memory_significance(
                content, self._conversation_context
            )

            if not memory_signals:
                logger.debug("No significant memory signals detected")
                return []

            created_memory_ids = []

            # Process each memory signal
            for signal in memory_signals:
                if signal.significance >= 0.4:  # Only store significant memories
                    try:
                        # Generate embedding for the memory content
                        embedding = await self.embedding_generator.generate(signal.content)

                        # Create enhanced metadata
                        metadata = {
                            "ai_analyzed": True,
                            "significance": signal.significance,
                            "context": signal.context,
                            "entities": signal.entities,
                            "reasoning": signal.reasoning,
                            "original_content": content,
                            "formation_timestamp": datetime.now().isoformat(),
                            "conversation_turn": len(self._conversation_context),
                        }

                        # Create memory object
                        memory = Memory(
                            id="",  # Will be generated by store
                            content=signal.content,
                            embedding=embedding,
                            memory_type=signal.memory_type,
                            importance=signal.significance,
                            metadata=metadata,
                        )

                        # Store memory
                        memory_id = await self.store.store(memory)

                        if memory_id:
                            created_memory_ids.append(memory_id)
                            logger.info(
                                f"Intelligently stored {signal.memory_type.value} memory: {memory_id[:8]}... "
                                f"(significance: {signal.significance:.2f}, context: {signal.context})"
                            )

                    except Exception as e:
                        logger.error(f"Failed to store intelligent memory signal: {e}")
                        continue

            if created_memory_ids:
                logger.info(
                    f"Intelligent analysis created {len(created_memory_ids)} memories from content analysis"
                )

            return created_memory_ids

        except Exception as e:
            logger.error(f"Intelligent memory analysis failed: {e}")
            return []

    async def recall(
        self,
        query: str,
        memory_types: list[MemoryType] | None = None,
        limit: int = 10,
        threshold: float = 0.7,
    ) -> list[Memory]:
        """Recall relevant memories based on query.

        Args:
            query: Search query
            memory_types: Types of memory to search (None = all)
            limit: Maximum memories to return
            threshold: Similarity threshold

        Returns:
            List of relevant memories
        """
        try:
            memories = []

            if memory_types:
                # Search specific memory types
                for memory_type in memory_types:
                    results = await self.store.search(
                        query=query, memory_type=memory_type, limit=limit, threshold=threshold
                    )
                    memories.extend(results)
            else:
                # Search all memory types
                memories = await self.store.search(query=query, limit=limit, threshold=threshold)

            # Update access times
            for memory in memories:
                memory.accessed_at = datetime.now()
                await self.store.update(memory)

            # Sort by relevance and importance
            memories.sort(key=lambda m: (m.importance, m.accessed_at.timestamp()), reverse=True)

            logger.info(f"Recalled {len(memories)} memories for query: {query[:50]}...")
            return memories[:limit]

        except Exception as e:
            logger.error(f"Failed to recall memories: {e}")
            return []

    async def consolidate(self):
        """Consolidate short-term memories into long-term storage."""
        try:
            # Get old short-term memories
            cutoff_time = datetime.now() - self.short_term_duration

            # Search for memories to consolidate
            all_short_term = await self.store.search(
                query="", memory_type=MemoryType.SHORT_TERM, limit=1000, threshold=0.0  # Get all
            )

            consolidated_count = 0

            for memory in all_short_term:
                # Check if memory should be consolidated
                if (
                    memory.created_at < cutoff_time
                    and memory.importance >= self.importance_threshold
                ):

                    # Convert to long-term memory
                    memory.memory_type = MemoryType.LONG_TERM
                    memory.metadata["consolidated_at"] = datetime.now().isoformat()

                    # Update in store
                    await self.store.update(memory)
                    consolidated_count += 1

                elif memory.created_at < cutoff_time:
                    # Remove unimportant old memories
                    await self.store.delete(memory.id)

            logger.info(f"Consolidated {consolidated_count} memories to long-term storage")

        except Exception as e:
            logger.error(f"Failed to consolidate memories: {e}")

    async def forget(self, memory_id: str) -> bool:
        """Forget a specific memory.

        Args:
            memory_id: ID of memory to forget

        Returns:
            True if successful
        """
        try:
            return await self.store.delete(memory_id)
        except Exception as e:
            logger.error(f"Failed to forget memory {memory_id}: {e}")
            return False

    async def clear_memories(self, memory_type: MemoryType | None = None) -> int:
        """Clear memories of a specific type or all memories.

        Args:
            memory_type: Type to clear (None = all)

        Returns:
            Number of memories cleared
        """
        try:
            return await self.store.clear(memory_type)
        except Exception as e:
            logger.error(f"Failed to clear memories: {e}")
            return 0

    async def get_memory_stats(self) -> dict[str, Any]:
        """Get statistics about stored memories.

        Returns:
            Dictionary with memory statistics
        """
        try:
            stats = {"total": 0, "by_type": {}, "avg_importance": 0.0, "storage_size": 0}

            total_importance = 0.0

            for memory_type in MemoryType:
                memories = await self.store.search(
                    query="", memory_type=memory_type, limit=10000, threshold=0.0
                )

                count = len(memories)
                stats["by_type"][memory_type.value] = count
                stats["total"] += count

                if memories:
                    total_importance += sum(m.importance for m in memories)

            if stats["total"] > 0:
                stats["avg_importance"] = total_importance / stats["total"]

            return stats

        except Exception as e:
            logger.error(f"Failed to get memory stats: {e}")
            return {"error": str(e)}

    def _classify_memory_type(self, content: str, default_type: MemoryType) -> MemoryType:
        """Intelligently classify memory type based on content."""
        content_lower = content.lower()

        # Keywords for different memory types
        episodic_keywords = [
            "yesterday",
            "today",
            "tomorrow",
            "last week",
            "next week",
            "when",
            "happened",
            "event",
            "meeting",
            "conversation",
            "said",
            "told",
            "experience",
            "remember when",
            "that time",
            "during",
            "while",
        ]

        semantic_keywords = [
            "fact",
            "definition",
            "concept",
            "theory",
            "principle",
            "rule",
            "always",
            "never",
            "generally",
            "usually",
            "knowledge",
            "learn",
            "understand",
            "means",
            "refers to",
            "defined as",
        ]

        long_term_keywords = [
            "important",
            "remember",
            "preference",
            "setting",
            "configuration",
            "always do",
            "never do",
            "my",
            "personal",
            "profile",
            "about me",
            "name is",
            "call me",
            "i am",
            "phone number",
            "email",
            "address",
        ]

        # Count keyword matches
        episodic_score = sum(1 for keyword in episodic_keywords if keyword in content_lower)
        semantic_score = sum(1 for keyword in semantic_keywords if keyword in content_lower)
        long_term_score = sum(1 for keyword in long_term_keywords if keyword in content_lower)

        # Check for personal pronouns and time references
        personal_pronouns = ["i", "my", "me", "myself", "we", "our", "us"]
        time_references = ["am", "pm", "hour", "minute", "day", "week", "month", "year"]

        has_personal = any(pronoun in content_lower.split() for pronoun in personal_pronouns)
        has_time = any(time_ref in content_lower for time_ref in time_references)

        # Classification logic
        if episodic_score >= 2 or (has_personal and has_time):
            return MemoryType.EPISODIC
        elif semantic_score >= 2:
            return MemoryType.SEMANTIC
        elif long_term_score >= 1 or has_personal:
            return MemoryType.LONG_TERM
        else:
            return default_type  # Keep original classification

    def _calculate_importance(self, content: str, default_importance: float) -> float:
        """Calculate importance score based on content analysis."""
        base_importance = default_importance

        # Importance boosters
        importance_keywords = [
            "important",
            "critical",
            "urgent",
            "remember",
            "never forget",
            "always",
            "must",
            "required",
            "essential",
            "key",
            "vital",
        ]

        personal_keywords = [
            "password",
            "phone",
            "address",
            "email",
            "name",
            "birthday",
            "preference",
            "setting",
            "like",
            "dislike",
            "hate",
            "love",
            "name is",
            "call me",
            "i am",
            "my name",
            "about me",
        ]

        content_lower = content.lower()

        # Boost for importance keywords
        importance_boost = sum(0.1 for keyword in importance_keywords if keyword in content_lower)

        # Boost for personal information
        personal_boost = sum(0.15 for keyword in personal_keywords if keyword in content_lower)

        # Boost for longer, more detailed content
        length_boost = min(0.2, len(content) / 1000)  # Max 0.2 boost for very long content

        # Boost for structured information (lists, steps, etc.)
        structure_boost = (
            0.1 if any(marker in content for marker in ["\n-", "\n1.", "\n2.", ":\n"]) else 0
        )

        # Calculate final importance
        final_importance = min(
            1.0,
            base_importance + importance_boost + personal_boost + length_boost + structure_boost,
        )

        return round(final_importance, 2)

    def _analyze_content(self, content: str) -> dict[str, Any]:
        """Analyze content and extract useful metadata."""
        analysis = {}

        # Basic text analysis
        words = content.split()
        sentences = content.split(".") if "." in content else [content]

        analysis.update(
            {
                "sentence_count": len([s for s in sentences if s.strip()]),
                "avg_word_length": (
                    round(sum(len(word) for word in words) / len(words), 1) if words else 0
                ),
                "has_questions": "?" in content,
                "has_urls": "http" in content.lower() or "www." in content.lower(),
                "has_code": any(
                    marker in content for marker in ["def ", "class ", "import ", "function", "=>"]
                ),
                "language_detected": self._detect_language_hints(content),
            }
        )

        # Extract entities (simple approach)
        entities = self._extract_simple_entities(content)
        if entities:
            analysis["entities"] = entities

        return analysis

    def _detect_language_hints(self, content: str) -> str:
        """Detect programming language hints in content."""
        content_lower = content.lower()

        language_patterns = {
            "python": ["def ", "import ", "python", ".py", "pip"],
            "javascript": ["function", "const ", "let ", "var ", ".js", "npm"],
            "java": ["public class", "import java", ".java", "maven"],
            "cpp": ["#include", "int main", ".cpp", ".hpp"],
            "sql": ["select ", "from ", "where ", "insert ", "update "],
            "bash": ["#!/bin/bash", "chmod", "sudo", ".sh"],
        }

        for language, patterns in language_patterns.items():
            if any(pattern in content_lower for pattern in patterns):
                return language

        return "natural"

    def _extract_simple_entities(self, content: str) -> list[str]:
        """Extract simple entities like names, places, etc."""
        entities = []

        # Simple patterns for common entities
        import re

        # Email addresses
        emails = re.findall(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", content)
        entities.extend([f"email:{email}" for email in emails])

        # URLs
        urls = re.findall(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
            content,
        )
        entities.extend([f"url:{url}" for url in urls])

        # Phone numbers (simple pattern)
        phones = re.findall(r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b", content)
        entities.extend([f"phone:{phone}" for phone in phones])

        return entities[:10]  # Limit to prevent metadata bloat

    def _analyze_content_minimal(self, content: str) -> dict[str, Any]:
        """Minimal content analysis for speed."""
        analysis = {}

        # Basic analysis only
        words = content.split()
        analysis.update(
            {
                "word_count": len(words),
                "has_questions": "?" in content,
                "has_code": any(
                    marker in content for marker in ["def ", "class ", "import ", "function"]
                ),
            }
        )

        return analysis
